{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "In this workshop we provide a very short introduction to neural networks in Python. This is very far from a comprehensive coverage of the topic but can provide a quick start for those who wish to learn more about the topic in their own time. We will cover a classification and a regression taks using `keras` as our python package of choice. If you want to try and implement a NN from scratch there are several good online tutorials that can help you do so (see [here](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6) for example). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological inspiration\n",
    "The (for our purpose) smallest stand-alone element in the human brain is the neuron. Its understanding and computational recreation build the foundation for ANNs. A simplified image of a \"real\" neuron can be seen below\n",
    "\n",
    "![](bio_neuron.png)\n",
    "\n",
    "Dendrites are connecting to the axons (or \"outputs\") of other neurons, for instance nerves in the sensory system or other processing neurons. In the nucleus, these input signals are aggregated and forwarded through the axon. The axon terminals then connect to further neurons to build the neural network. The connection between axon terminal and dendrite is what we are calling a synapse. In the human brain, there are billions of neurons and $10^{14} - 10^{15}$ synapses in the human brain. If each synapse (or more precisely, its connection strength) would be represented by 8 bits or one byte, just storing these numbers would take 1000 TB already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational implementation\n",
    "To recreate neural networks artificially, neurons have to be defined. The common mathematical model used for this purpose is depicted below.\n",
    "\n",
    "![](math_neuron.jpeg)\n",
    "\n",
    "From a certain number of input synapses $x_i$, signals come in with a weight factor of $w_i$. This represents the strength of the synapse. In the _nuclues_ these weighted inputs are aggregated and a bias is added. (The bias is not shown in every model, but it does make the neural network more generalizable). After adding of the weighted inputs and the bias, everything is fed into a (non-linear) activation function. The output is then either fed forward to further neurons or is the output of your neural network. If there is only one neuron that takes direct inputs and whose output is your interest, the model is called a single-layer perceptron. Many of these neurons can create almost arbitrary logical connections and functions, making ANNs very powerful. In this case, we are talking about a multi-layer perceptron (MLP) model. \n",
    "\n",
    "![](mlp-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "The activation function is (to some degree) the hear of the neural network. Without a non-linear activation function, all hidden layers do not add any value, but are instead a complicated way to represent a liner model. Only with a non-linear activation function, ANNs can recreate non-linear hypothesis functions. In the beginning of research on the ANNs in the scope of AI, typically a unit step was used as activation function. The unit step is $0$ for inputs smaller than $0$ and $1$ otherwise. The idea behind this is to recreate the behavior of a biological neuron that _fires_ if a certain threshold of inputs is exceeded. Today, other activation functions are more typically used. This is linked to better mathematical qualities in terms of learning behavior and convergence. Some of the most popular activation functions are:\n",
    "\n",
    "Sigmoid: $\\sigma(z) = \\frac{1}{1+exp(-z)}$\n",
    "\n",
    "Hyperbolic tangent: $\\sigma(z) = \\frac{2}{1+exp(-2z)} -1 $\n",
    "\n",
    "ReLU (Rectified Linear Unit): $\\sigma(z) = z\\quad  for\\ z>0,\\ 0\\ otherwise$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "As learning of ANNs is a non-trivial mathematical task, we are only aiming for an intuitive understanding here. Let's have a look at our complete MLP first.\n",
    "\n",
    "The general learning tasks consists of two steps, which are repeated until the algorithm converges:\n",
    "1. __Feedforward: Calculating the predicted output ŷ and the associated loss__. At first, we randomly assign values for the weights (and the biases). Based on the input features, the output value is calculated.\n",
    "2. __Backpropagation: Updating the weights W and biases b__. If the output value and the target value differ, the weights and biases are updated. To do this, it is calculated how much each weight and bias contributes to the error. Proportionally to this, they are then corrected (scaled with a small learning factor). In this sense, the updating rule has some similarity to gradient descent, only that is is propagated through the entire network, which is why this algorithm is called backpropagation.\n",
    "\n",
    "The training routine for a simple 2-layered MLP is shown in the below figure:\n",
    "\n",
    "![](training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "The main hyperparameters of an MLP are: \n",
    "\n",
    "1. Number of hidden layers\n",
    "1. Number of nodes\n",
    "4. Activation function\n",
    "\n",
    "The number of hidden layers and number of nodes (its activation function could be understood as a hyperparameter, but that is typically not done). The more layers and nodes there are (and the denser the network is, i.e. the more edges have a non-zero weight) the harder it gets to learn the model. That's the reason why bigger ANNs are normally not trained on a local computer anymore, but on specialized computers. Furthermore, there are additional libraries for python to improve the efficiency of ANNs, e.g. TensorFlow or Keras, which we take a first look at in today's tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` is one of the most popular Deep Learning libraries. `Tensorflow` and `Theano` are the most used numerical platforms in Python to build Deep Learning algorithms but they can be quite complex and difficult to use.\n",
    "\n",
    "Keras, by contrast is easy to use and is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, Theano, and MXNet. The full documentation of the keras API can be found [here](https://keras.io).\n",
    "\n",
    "Note that `scikit learn` also features an MLP implementation (see [here](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)). Yet, `keras` has advanced to be one of the most popular frameworks used in practice, which is why we focus on it in this short tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` sits on top of `TensorFlow`, therefore we fist need to intall the latter library. To do so execute the following command:\n",
    "\n",
    "`conda install -c conda-forge tensorflow`\n",
    "\n",
    "When you are done use the following command via the command line to install `keras`.\n",
    "\n",
    "`conda install -c conda-forge keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks for classification in `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stay with our example, we will build a NN that predicts the class of a breast cancer by categorizing it as either malignant or begnign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# supress versioning warnings of keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import keras libraries\n",
    "\n",
    "#from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Preparation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842302</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842517</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "id                                                                       \n",
       "842302         M        17.99         10.38           122.8     1001.0   \n",
       "842517         M        20.57         17.77           132.9     1326.0   \n",
       "\n",
       "        smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "id                                                          \n",
       "842302          0.11840           0.27760          0.3001   \n",
       "842517          0.08474           0.07864          0.0869   \n",
       "\n",
       "        concave points_mean  symmetry_mean     ...       texture_worst  \\\n",
       "id                                             ...                       \n",
       "842302              0.14710         0.2419     ...               17.33   \n",
       "842517              0.07017         0.1812     ...               23.41   \n",
       "\n",
       "        perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "id                                                                         \n",
       "842302            184.6      2019.0            0.1622             0.6656   \n",
       "842517            158.8      1956.0            0.1238             0.1866   \n",
       "\n",
       "        concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "id                                                              \n",
       "842302           0.7119                0.2654          0.4601   \n",
       "842517           0.2416                0.1860          0.2750   \n",
       "\n",
       "        fractal_dimension_worst  Unnamed: 32  \n",
       "id                                            \n",
       "842302                  0.11890          NaN  \n",
       "842517                  0.08902          NaN  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "cancer_df = pd.read_csv(\"breast_cancer.csv\", index_col = \"id\")\n",
    "cancer_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and Y\n",
    "X = cancer_df.iloc[:,1:31] # include full feature vector\n",
    "y = cancer_df[\"diagnosis\"]\n",
    "\n",
    "\n",
    "# encode categorical target verctor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initializing and Training the ANN__\n",
    "\n",
    "We start by defining the type of model we want to build. There are two types of models available in Keras: the [Sequential model](https://keras.io/models/sequential/) and the Model class used with [functional API](https://keras.io/models/model/). Then we simply add the input-, 2 hidden- and output-layers.\n",
    "\n",
    "Between them, we are using [dropout](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) to prevent overfitting (dropout rate should be between 20% and 50%).\n",
    "\n",
    "![](dropout.png)\n",
    "\n",
    "At every layer, we use “Dense” which means that the nodes are fully connected.\n",
    "\n",
    "The input-layer takes 30 inputs (because our feature vector includes 30 features) as input and outputs it with a shape of 16, which is the number of nodes in the first hidden layer that we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass the following parameters:\n",
    "\n",
    "- input_shape - number of columns of the dataset (only for input layer)\n",
    "\n",
    "- units - number of neurons and dimensionality of outputs to be fed to the next layer, if any\n",
    "\n",
    "- activation - activation function which is ReLU in this case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the input layer and the first hidden layer (with 30 nodes)\n",
    "classifier.add(Dense(input_shape = (30,), \n",
    "                     units=30,          #dimensionality of the output space (#nodes in the first hidden layer)\n",
    "                     activation='relu'))\n",
    "\n",
    "# Adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an additional second layer, also with 15 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units= 15,\n",
    "                     activation='relu'))\n",
    "\n",
    "# Adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add the output layer. Since we perform a binary classification, a single output node suffices. We use a sigmoidal activation function for this last node which is often used when dealing with binary classfication problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units= 1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compile the model to configure it for training. We add the following parameters:\n",
    "- `optimizer`: Here we use the adam optimizer, an optimizer with higher performance in many cases than stochastic gradient descent (SGD). See [here](https://keras.io/optimizers/) for a list of all optimzers implemented in `keras`.\n",
    "- `loss`: specifies the loss to be minimized. In this example we use binary crossentropy, a common loss for binary classification tasks. See [here](https://keras.io/losses/) for an overview of available losses in keras \n",
    "- `metrics`:  metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model and merely function as indicator of model performance to the data scientist. An overview ov available metrics can be found [here](https://keras.io/metrics/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer=\"adam\",    # Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments\n",
    "              loss=\"binary_crossentropy\",  # this is a good loss for binary classification\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 1,411\n",
      "Trainable params: 1,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to train our model. We do this with a batch_size of 50 and for 100 epochs.\n",
    "\n",
    "- `batch_size` defines the number of samples that will be propagated through the network \n",
    "- `epoch` defines the number of iteration over the entire training data\n",
    "\n",
    "In general a larger batch-size results in faster training, but does not always converge fast. A smaller batch-size is slower in training but it can converge faster. This is definitely problem dependent and you need to try out a few different values (the standard batch-size is 32). The same goes for the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "398/398 [==============================] - 0s 608us/step - loss: 0.5860 - accuracy: 0.7814\n",
      "Epoch 2/100\n",
      "398/398 [==============================] - 0s 240us/step - loss: 0.4788 - accuracy: 0.8593\n",
      "Epoch 3/100\n",
      "398/398 [==============================] - 0s 141us/step - loss: 0.4179 - accuracy: 0.8794\n",
      "Epoch 4/100\n",
      "398/398 [==============================] - 0s 147us/step - loss: 0.3653 - accuracy: 0.8970\n",
      "Epoch 5/100\n",
      "398/398 [==============================] - 0s 131us/step - loss: 0.3154 - accuracy: 0.9045\n",
      "Epoch 6/100\n",
      "398/398 [==============================] - 0s 127us/step - loss: 0.2776 - accuracy: 0.8995\n",
      "Epoch 7/100\n",
      "398/398 [==============================] - 0s 69us/step - loss: 0.2356 - accuracy: 0.9221\n",
      "Epoch 8/100\n",
      "398/398 [==============================] - 0s 109us/step - loss: 0.2169 - accuracy: 0.9271\n",
      "Epoch 9/100\n",
      "398/398 [==============================] - 0s 72us/step - loss: 0.2089 - accuracy: 0.9347\n",
      "Epoch 10/100\n",
      "398/398 [==============================] - 0s 64us/step - loss: 0.1891 - accuracy: 0.9397\n",
      "Epoch 11/100\n",
      "398/398 [==============================] - 0s 183us/step - loss: 0.1692 - accuracy: 0.9497\n",
      "Epoch 12/100\n",
      "398/398 [==============================] - 0s 156us/step - loss: 0.1511 - accuracy: 0.9548\n",
      "Epoch 13/100\n",
      "398/398 [==============================] - 0s 83us/step - loss: 0.1488 - accuracy: 0.9447\n",
      "Epoch 14/100\n",
      "398/398 [==============================] - 0s 64us/step - loss: 0.1333 - accuracy: 0.9598\n",
      "Epoch 15/100\n",
      "398/398 [==============================] - 0s 74us/step - loss: 0.1290 - accuracy: 0.9598\n",
      "Epoch 16/100\n",
      "398/398 [==============================] - 0s 96us/step - loss: 0.1307 - accuracy: 0.9598\n",
      "Epoch 17/100\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.1238 - accuracy: 0.9573\n",
      "Epoch 18/100\n",
      "398/398 [==============================] - 0s 71us/step - loss: 0.1205 - accuracy: 0.9573\n",
      "Epoch 19/100\n",
      "398/398 [==============================] - 0s 69us/step - loss: 0.1109 - accuracy: 0.9623\n",
      "Epoch 20/100\n",
      "398/398 [==============================] - 0s 76us/step - loss: 0.1064 - accuracy: 0.9648\n",
      "Epoch 21/100\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.1016 - accuracy: 0.9724\n",
      "Epoch 22/100\n",
      "398/398 [==============================] - 0s 75us/step - loss: 0.0910 - accuracy: 0.9724\n",
      "Epoch 23/100\n",
      "398/398 [==============================] - 0s 251us/step - loss: 0.0929 - accuracy: 0.9673\n",
      "Epoch 24/100\n",
      "398/398 [==============================] - 0s 81us/step - loss: 0.0867 - accuracy: 0.9698\n",
      "Epoch 25/100\n",
      "398/398 [==============================] - 0s 59us/step - loss: 0.0894 - accuracy: 0.9774\n",
      "Epoch 26/100\n",
      "398/398 [==============================] - 0s 58us/step - loss: 0.0832 - accuracy: 0.9724\n",
      "Epoch 27/100\n",
      "398/398 [==============================] - 0s 129us/step - loss: 0.0795 - accuracy: 0.9799\n",
      "Epoch 28/100\n",
      "398/398 [==============================] - 0s 66us/step - loss: 0.0831 - accuracy: 0.9799\n",
      "Epoch 29/100\n",
      "398/398 [==============================] - 0s 90us/step - loss: 0.0735 - accuracy: 0.9799\n",
      "Epoch 30/100\n",
      "398/398 [==============================] - 0s 163us/step - loss: 0.0795 - accuracy: 0.9724\n",
      "Epoch 31/100\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.0829 - accuracy: 0.9774\n",
      "Epoch 32/100\n",
      "398/398 [==============================] - 0s 72us/step - loss: 0.0785 - accuracy: 0.9774\n",
      "Epoch 33/100\n",
      "398/398 [==============================] - 0s 90us/step - loss: 0.0792 - accuracy: 0.9774\n",
      "Epoch 34/100\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.0585 - accuracy: 0.9874\n",
      "Epoch 35/100\n",
      "398/398 [==============================] - 0s 214us/step - loss: 0.0687 - accuracy: 0.9774\n",
      "Epoch 36/100\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.0670 - accuracy: 0.9799\n",
      "Epoch 37/100\n",
      "398/398 [==============================] - 0s 94us/step - loss: 0.0600 - accuracy: 0.9824\n",
      "Epoch 38/100\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.0679 - accuracy: 0.9799\n",
      "Epoch 39/100\n",
      "398/398 [==============================] - 0s 77us/step - loss: 0.0601 - accuracy: 0.9824\n",
      "Epoch 40/100\n",
      "398/398 [==============================] - 0s 67us/step - loss: 0.0550 - accuracy: 0.9874\n",
      "Epoch 41/100\n",
      "398/398 [==============================] - 0s 128us/step - loss: 0.0565 - accuracy: 0.9849\n",
      "Epoch 42/100\n",
      "398/398 [==============================] - 0s 106us/step - loss: 0.0551 - accuracy: 0.9799\n",
      "Epoch 43/100\n",
      "398/398 [==============================] - 0s 69us/step - loss: 0.0510 - accuracy: 0.9849\n",
      "Epoch 44/100\n",
      "398/398 [==============================] - 0s 62us/step - loss: 0.0490 - accuracy: 0.9824\n",
      "Epoch 45/100\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.0577 - accuracy: 0.9849\n",
      "Epoch 46/100\n",
      "398/398 [==============================] - 0s 67us/step - loss: 0.0559 - accuracy: 0.9849\n",
      "Epoch 47/100\n",
      "398/398 [==============================] - 0s 67us/step - loss: 0.0497 - accuracy: 0.9849\n",
      "Epoch 48/100\n",
      "398/398 [==============================] - 0s 76us/step - loss: 0.0529 - accuracy: 0.9874\n",
      "Epoch 49/100\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.0468 - accuracy: 0.9874\n",
      "Epoch 50/100\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.0441 - accuracy: 0.9849\n",
      "Epoch 51/100\n",
      "398/398 [==============================] - 0s 69us/step - loss: 0.0430 - accuracy: 0.9899\n",
      "Epoch 52/100\n",
      "398/398 [==============================] - 0s 72us/step - loss: 0.0488 - accuracy: 0.9849\n",
      "Epoch 53/100\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.0492 - accuracy: 0.9824\n",
      "Epoch 54/100\n",
      "398/398 [==============================] - 0s 69us/step - loss: 0.0407 - accuracy: 0.9899\n",
      "Epoch 55/100\n",
      "398/398 [==============================] - 0s 126us/step - loss: 0.0504 - accuracy: 0.9824\n",
      "Epoch 56/100\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.0429 - accuracy: 0.9874\n",
      "Epoch 57/100\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.0420 - accuracy: 0.9849\n",
      "Epoch 58/100\n",
      "398/398 [==============================] - 0s 87us/step - loss: 0.0409 - accuracy: 0.9899\n",
      "Epoch 59/100\n",
      "398/398 [==============================] - 0s 81us/step - loss: 0.0414 - accuracy: 0.9925\n",
      "Epoch 60/100\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.0365 - accuracy: 0.9849\n",
      "Epoch 61/100\n",
      "398/398 [==============================] - 0s 59us/step - loss: 0.0363 - accuracy: 0.9899\n",
      "Epoch 62/100\n",
      "398/398 [==============================] - 0s 72us/step - loss: 0.0415 - accuracy: 0.9874\n",
      "Epoch 63/100\n",
      "398/398 [==============================] - 0s 77us/step - loss: 0.0347 - accuracy: 0.9925\n",
      "Epoch 64/100\n",
      "398/398 [==============================] - 0s 67us/step - loss: 0.0418 - accuracy: 0.9899\n",
      "Epoch 65/100\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.0316 - accuracy: 0.9925\n",
      "Epoch 66/100\n",
      "398/398 [==============================] - 0s 72us/step - loss: 0.0358 - accuracy: 0.9925\n",
      "Epoch 67/100\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.0383 - accuracy: 0.9925\n",
      "Epoch 68/100\n",
      "398/398 [==============================] - 0s 55us/step - loss: 0.0298 - accuracy: 0.9925\n",
      "Epoch 69/100\n",
      "398/398 [==============================] - 0s 66us/step - loss: 0.0294 - accuracy: 0.9925\n",
      "Epoch 70/100\n",
      "398/398 [==============================] - 0s 65us/step - loss: 0.0310 - accuracy: 0.9925\n",
      "Epoch 71/100\n",
      "398/398 [==============================] - 0s 62us/step - loss: 0.0388 - accuracy: 0.9874\n",
      "Epoch 72/100\n",
      "398/398 [==============================] - 0s 84us/step - loss: 0.0310 - accuracy: 0.9925\n",
      "Epoch 73/100\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.0314 - accuracy: 0.9925\n",
      "Epoch 74/100\n",
      "398/398 [==============================] - 0s 73us/step - loss: 0.0344 - accuracy: 0.9899\n",
      "Epoch 75/100\n",
      "398/398 [==============================] - 0s 90us/step - loss: 0.0327 - accuracy: 0.9899\n",
      "Epoch 76/100\n",
      "398/398 [==============================] - 0s 116us/step - loss: 0.0288 - accuracy: 0.9950\n",
      "Epoch 77/100\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.0252 - accuracy: 0.9975\n",
      "Epoch 78/100\n",
      "398/398 [==============================] - 0s 94us/step - loss: 0.0275 - accuracy: 0.9950\n",
      "Epoch 79/100\n",
      "398/398 [==============================] - 0s 71us/step - loss: 0.0295 - accuracy: 0.9899\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 84us/step - loss: 0.0300 - accuracy: 0.9925\n",
      "Epoch 81/100\n",
      "398/398 [==============================] - 0s 296us/step - loss: 0.0372 - accuracy: 0.9874\n",
      "Epoch 82/100\n",
      "398/398 [==============================] - 0s 81us/step - loss: 0.0248 - accuracy: 0.9950\n",
      "Epoch 83/100\n",
      "398/398 [==============================] - 0s 89us/step - loss: 0.0258 - accuracy: 0.9925\n",
      "Epoch 84/100\n",
      "398/398 [==============================] - 0s 82us/step - loss: 0.0248 - accuracy: 0.9899\n",
      "Epoch 85/100\n",
      "398/398 [==============================] - 0s 70us/step - loss: 0.0259 - accuracy: 0.9925\n",
      "Epoch 86/100\n",
      "398/398 [==============================] - 0s 56us/step - loss: 0.0255 - accuracy: 0.9925\n",
      "Epoch 87/100\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.0268 - accuracy: 0.9899\n",
      "Epoch 88/100\n",
      "398/398 [==============================] - 0s 59us/step - loss: 0.0273 - accuracy: 0.9899\n",
      "Epoch 89/100\n",
      "398/398 [==============================] - 0s 115us/step - loss: 0.0264 - accuracy: 0.9925\n",
      "Epoch 90/100\n",
      "398/398 [==============================] - 0s 72us/step - loss: 0.0231 - accuracy: 0.9925\n",
      "Epoch 91/100\n",
      "398/398 [==============================] - 0s 68us/step - loss: 0.0192 - accuracy: 0.9950\n",
      "Epoch 92/100\n",
      "398/398 [==============================] - 0s 78us/step - loss: 0.0215 - accuracy: 0.9975\n",
      "Epoch 93/100\n",
      "398/398 [==============================] - 0s 83us/step - loss: 0.0219 - accuracy: 0.9925\n",
      "Epoch 94/100\n",
      "398/398 [==============================] - 0s 79us/step - loss: 0.0170 - accuracy: 0.9975\n",
      "Epoch 95/100\n",
      "398/398 [==============================] - 0s 93us/step - loss: 0.0227 - accuracy: 0.9899\n",
      "Epoch 96/100\n",
      "398/398 [==============================] - 0s 86us/step - loss: 0.0201 - accuracy: 0.9950\n",
      "Epoch 97/100\n",
      "398/398 [==============================] - 0s 150us/step - loss: 0.0210 - accuracy: 0.9950\n",
      "Epoch 98/100\n",
      "398/398 [==============================] - 0s 130us/step - loss: 0.0212 - accuracy: 0.9925\n",
      "Epoch 99/100\n",
      "398/398 [==============================] - 0s 93us/step - loss: 0.0182 - accuracy: 0.9925\n",
      "Epoch 100/100\n",
      "398/398 [==============================] - 0s 60us/step - loss: 0.0214 - accuracy: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f98dd636b70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size=50, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[107   1]\n",
      " [  1  62]]\n",
      "\n",
      "Accuracy\n",
      "0.9883\n",
      "\n",
      "Precision\n",
      "0.9841\n"
     ]
    }
   ],
   "source": [
    "# Report classification performance on test set\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "accuracy_score = accuracy_score(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "precision_score = precision_score(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix)\n",
    "print()\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score.round(decimals=4))\n",
    "print()\n",
    "print(\"Precision\")\n",
    "print(precision_score.round(decimals=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks for regression in `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can also be trained for regression tasks. The logic is exactly the same, yet some of the parameters, such as loss, metrics, input and ouput as well as typical activation functions might have to be adapted to the specific case. There are a range of very good tutorial online which we encourage you to take a look at (for example [here](https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/)). \n",
    "\n",
    "We will cover a simple implimentation on the `Diamonds` dataset. The objective in this task is to predict the price of a particular dimond based on different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>402</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.03</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28299</th>\n",
       "      <td>0.30</td>\n",
       "      <td>Good</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>432</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28275</th>\n",
       "      <td>0.26</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.5</td>\n",
       "      <td>53.0</td>\n",
       "      <td>362</td>\n",
       "      <td>4.09</td>\n",
       "      <td>4.13</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>0.74</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>62.3</td>\n",
       "      <td>57.0</td>\n",
       "      <td>3170</td>\n",
       "      <td>5.79</td>\n",
       "      <td>5.77</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50059</th>\n",
       "      <td>0.57</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>VS2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2202</td>\n",
       "      <td>5.39</td>\n",
       "      <td>5.43</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z\n",
       "30      0.23  Very Good     F     VS1   60.0   57.0    402  4.00  4.03  2.41\n",
       "28299   0.30       Good     H     SI1   63.7   57.0    432  4.26  4.28  2.72\n",
       "28275   0.26      Ideal     H     SI2   62.5   53.0    362  4.09  4.13  2.57\n",
       "2338    0.74      Ideal     F     VS1   62.3   57.0   3170  5.79  5.77  3.60\n",
       "50059   0.57      Ideal     E     VS2   61.0   54.0   2202  5.39  5.43  3.30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds = sns.load_dataset('diamonds')\n",
    "diamonds.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.797940</td>\n",
       "      <td>61.749405</td>\n",
       "      <td>57.457184</td>\n",
       "      <td>3932.799722</td>\n",
       "      <td>5.731157</td>\n",
       "      <td>5.734526</td>\n",
       "      <td>3.538734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.474011</td>\n",
       "      <td>1.432621</td>\n",
       "      <td>2.234491</td>\n",
       "      <td>3989.439738</td>\n",
       "      <td>1.121761</td>\n",
       "      <td>1.142135</td>\n",
       "      <td>0.705699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>5324.250000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.010000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>18823.000000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>31.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table         price             x  \\\n",
       "count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n",
       "mean       0.797940     61.749405     57.457184   3932.799722      5.731157   \n",
       "std        0.474011      1.432621      2.234491   3989.439738      1.121761   \n",
       "min        0.200000     43.000000     43.000000    326.000000      0.000000   \n",
       "25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n",
       "50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n",
       "75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \n",
       "max        5.010000     79.000000     95.000000  18823.000000     10.740000   \n",
       "\n",
       "                  y             z  \n",
       "count  53940.000000  53940.000000  \n",
       "mean       5.734526      3.538734  \n",
       "std        1.142135      0.705699  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.720000      2.910000  \n",
       "50%        5.710000      3.530000  \n",
       "75%        6.540000      4.040000  \n",
       "max       58.900000     31.800000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "sns.heatmap(diamonds.isna(), ax=ax,\n",
    "           vmin=0, vmax=1, cmap=\"Reds\",\n",
    "           cbar_kws={\"ticks\":[0,1]})\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Show Missing Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dummy variables\n",
    "\n",
    "Since in the diamond dataset we have three categorical input features, we need to convert them into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut_Fair</th>\n",
       "      <th>cut_Good</th>\n",
       "      <th>cut_Ideal</th>\n",
       "      <th>...</th>\n",
       "      <th>color_I</th>\n",
       "      <th>color_J</th>\n",
       "      <th>clarity_I1</th>\n",
       "      <th>clarity_IF</th>\n",
       "      <th>clarity_SI1</th>\n",
       "      <th>clarity_SI2</th>\n",
       "      <th>clarity_VS1</th>\n",
       "      <th>clarity_VS2</th>\n",
       "      <th>clarity_VVS1</th>\n",
       "      <th>clarity_VVS2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21250</th>\n",
       "      <td>1.19</td>\n",
       "      <td>60.9</td>\n",
       "      <td>53.0</td>\n",
       "      <td>9403</td>\n",
       "      <td>6.88</td>\n",
       "      <td>6.92</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32681</th>\n",
       "      <td>0.31</td>\n",
       "      <td>61.2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>802</td>\n",
       "      <td>4.33</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6206</th>\n",
       "      <td>1.00</td>\n",
       "      <td>62.2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>3998</td>\n",
       "      <td>6.29</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8889</th>\n",
       "      <td>1.06</td>\n",
       "      <td>57.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4488</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.71</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10495</th>\n",
       "      <td>1.02</td>\n",
       "      <td>60.2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4798</td>\n",
       "      <td>6.56</td>\n",
       "      <td>6.49</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  depth  table  price     x     y     z  cut_Fair  cut_Good  \\\n",
       "21250   1.19   60.9   53.0   9403  6.88  6.92  4.20         0         0   \n",
       "32681   0.31   61.2   60.0    802  4.33  4.30  2.64         0         0   \n",
       "6206    1.00   62.2   57.0   3998  6.29  6.25  3.90         0         0   \n",
       "8889    1.06   57.0   64.0   4488  6.80  6.71  3.85         0         1   \n",
       "10495   1.02   60.2   57.0   4798  6.56  6.49  3.93         0         0   \n",
       "\n",
       "       cut_Ideal      ...       color_I  color_J  clarity_I1  clarity_IF  \\\n",
       "21250          1      ...             0        0           0           0   \n",
       "32681          0      ...             0        0           0           0   \n",
       "6206           0      ...             0        0           0           0   \n",
       "8889           0      ...             0        0           0           0   \n",
       "10495          1      ...             1        0           0           0   \n",
       "\n",
       "       clarity_SI1  clarity_SI2  clarity_VS1  clarity_VS2  clarity_VVS1  \\\n",
       "21250            0            0            0            0             0   \n",
       "32681            0            0            1            0             0   \n",
       "6206             0            1            0            0             0   \n",
       "8889             1            0            0            0             0   \n",
       "10495            0            0            1            0             0   \n",
       "\n",
       "       clarity_VVS2  \n",
       "21250             1  \n",
       "32681             0  \n",
       "6206              0  \n",
       "8889              0  \n",
       "10495             0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds = pd.get_dummies(diamonds)\n",
    "diamonds.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining inputs and output\n",
    "\n",
    "X = diamonds.drop(\"price\", axis=1)\n",
    "y = diamonds[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing training data\n",
    "\n",
    "st_scaler = StandardScaler()\n",
    "st_scaler.fit(X_train)\n",
    "X_train_scaled = st_scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential(\n",
    "    [Dense(36, activation=\"relu\", input_shape=[X_train.shape[1]]),\n",
    "    Dense(36, activation=\"relu\"),\n",
    "     Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "\n",
    "model.compile(loss='mse',\n",
    "             optimizer=\"adam\",\n",
    "             metrics=[\"mae\", \"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 36)                972       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 37        \n",
      "=================================================================\n",
      "Total params: 2,341\n",
      "Trainable params: 2,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30206 samples, validate on 7552 samples\n",
      "Epoch 1/20\n",
      "30206/30206 [==============================] - 2s 75us/step - loss: 17002764.9398 - mae: 2680.7319 - mse: 17002772.0000 - val_loss: 3633166.2887 - val_mae: 1225.3829 - val_mse: 3633165.7500\n",
      "Epoch 2/20\n",
      "30206/30206 [==============================] - 3s 98us/step - loss: 1556326.4028 - mae: 839.7398 - mse: 1556324.7500 - val_loss: 1757297.8276 - val_mae: 670.3343 - val_mse: 1757298.0000\n",
      "Epoch 3/20\n",
      "30206/30206 [==============================] - 2s 57us/step - loss: 857932.2936 - mae: 613.8824 - mse: 857932.1250 - val_loss: 1625917.0036 - val_mae: 616.7448 - val_mse: 1625917.0000\n",
      "Epoch 4/20\n",
      "30206/30206 [==============================] - 2s 62us/step - loss: 765727.0263 - mae: 572.6487 - mse: 765726.8125 - val_loss: 1583233.6880 - val_mae: 580.4091 - val_mse: 1583233.3750\n",
      "Epoch 5/20\n",
      "30206/30206 [==============================] - 2s 62us/step - loss: 708789.1788 - mae: 534.1184 - mse: 708789.1875 - val_loss: 1559925.7113 - val_mae: 540.7899 - val_mse: 1559926.5000\n",
      "Epoch 6/20\n",
      "30206/30206 [==============================] - 2s 62us/step - loss: 659086.9952 - mae: 497.7286 - mse: 659086.6250 - val_loss: 1548469.2735 - val_mae: 507.9236 - val_mse: 1548470.1250\n",
      "Epoch 7/20\n",
      "30206/30206 [==============================] - 2s 58us/step - loss: 616742.4079 - mae: 466.6969 - mse: 616741.9375 - val_loss: 1519841.9847 - val_mae: 471.8011 - val_mse: 1519842.1250\n",
      "Epoch 8/20\n",
      "30206/30206 [==============================] - 3s 91us/step - loss: 579331.1367 - mae: 438.3134 - mse: 579330.9375 - val_loss: 1521988.6477 - val_mae: 449.6484 - val_mse: 1521988.8750\n",
      "Epoch 9/20\n",
      "30206/30206 [==============================] - 3s 98us/step - loss: 550205.0974 - mae: 418.0928 - mse: 550205.2500 - val_loss: 1564564.3069 - val_mae: 433.1621 - val_mse: 1564563.5000\n",
      "Epoch 10/20\n",
      "30206/30206 [==============================] - 2s 55us/step - loss: 526310.5521 - mae: 400.8570 - mse: 526310.5625 - val_loss: 1550820.8404 - val_mae: 416.6625 - val_mse: 1550820.5000\n",
      "Epoch 11/20\n",
      "30206/30206 [==============================] - 2s 52us/step - loss: 506367.0337 - mae: 388.2887 - mse: 506367.1250 - val_loss: 1571376.7423 - val_mae: 406.7565 - val_mse: 1571376.2500\n",
      "Epoch 12/20\n",
      "30206/30206 [==============================] - 2s 52us/step - loss: 491988.0458 - mae: 379.9774 - mse: 491988.0000 - val_loss: 1628800.7476 - val_mae: 399.1452 - val_mse: 1628800.3750\n",
      "Epoch 13/20\n",
      "30206/30206 [==============================] - 2s 52us/step - loss: 480250.1413 - mae: 373.7391 - mse: 480249.9688 - val_loss: 1623106.6729 - val_mae: 391.2279 - val_mse: 1623106.1250\n",
      "Epoch 14/20\n",
      "30206/30206 [==============================] - 2s 54us/step - loss: 468111.6228 - mae: 367.8248 - mse: 468111.8750 - val_loss: 1630332.3303 - val_mae: 386.7135 - val_mse: 1630331.5000\n",
      "Epoch 15/20\n",
      "30206/30206 [==============================] - 2s 64us/step - loss: 459787.8916 - mae: 362.7392 - mse: 459787.9375 - val_loss: 1662621.2570 - val_mae: 383.5308 - val_mse: 1662621.0000\n",
      "Epoch 16/20\n",
      "30206/30206 [==============================] - 2s 52us/step - loss: 450718.2793 - mae: 359.0037 - mse: 450718.5312 - val_loss: 1759382.2185 - val_mae: 384.8977 - val_mse: 1759382.3750\n",
      "Epoch 17/20\n",
      "30206/30206 [==============================] - 2s 52us/step - loss: 445290.2674 - mae: 356.0935 - mse: 445290.0312 - val_loss: 1725214.3729 - val_mae: 375.1790 - val_mse: 1725214.8750\n",
      "Epoch 18/20\n",
      "30206/30206 [==============================] - 2s 52us/step - loss: 437974.6203 - mae: 353.0016 - mse: 437974.3125 - val_loss: 1742416.2543 - val_mae: 372.3768 - val_mse: 1742416.2500\n",
      "Epoch 19/20\n",
      "30206/30206 [==============================] - 2s 52us/step - loss: 432671.3851 - mae: 349.5672 - mse: 432671.2812 - val_loss: 1755289.7385 - val_mae: 371.0044 - val_mse: 1755290.1250\n",
      "Epoch 20/20\n",
      "30206/30206 [==============================] - 2s 52us/step - loss: 428529.9368 - mae: 347.8322 - mse: 428530.1875 - val_loss: 1756581.4932 - val_mae: 369.3407 - val_mse: 1756581.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train.values,\n",
    "                   epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3022.7617],\n",
       "       [1534.2948],\n",
       "       [1993.4548],\n",
       "       [4957.7607],\n",
       "       [3888.232 ],\n",
       "       [2012.2579],\n",
       "       [6453.5303],\n",
       "       [8533.8   ],\n",
       "       [1415.282 ],\n",
       "       [2898.1023]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train_scaled[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.633166e+06</td>\n",
       "      <td>1225.382935</td>\n",
       "      <td>3633165.750</td>\n",
       "      <td>1.700276e+07</td>\n",
       "      <td>2680.731934</td>\n",
       "      <td>1.700277e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.757298e+06</td>\n",
       "      <td>670.334290</td>\n",
       "      <td>1757298.000</td>\n",
       "      <td>1.556326e+06</td>\n",
       "      <td>839.739807</td>\n",
       "      <td>1.556325e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.625917e+06</td>\n",
       "      <td>616.744751</td>\n",
       "      <td>1625917.000</td>\n",
       "      <td>8.579323e+05</td>\n",
       "      <td>613.882446</td>\n",
       "      <td>8.579321e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.583234e+06</td>\n",
       "      <td>580.409119</td>\n",
       "      <td>1583233.375</td>\n",
       "      <td>7.657270e+05</td>\n",
       "      <td>572.648682</td>\n",
       "      <td>7.657268e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.559926e+06</td>\n",
       "      <td>540.789917</td>\n",
       "      <td>1559926.500</td>\n",
       "      <td>7.087892e+05</td>\n",
       "      <td>534.118408</td>\n",
       "      <td>7.087892e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.548469e+06</td>\n",
       "      <td>507.923553</td>\n",
       "      <td>1548470.125</td>\n",
       "      <td>6.590870e+05</td>\n",
       "      <td>497.728638</td>\n",
       "      <td>6.590866e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.519842e+06</td>\n",
       "      <td>471.801086</td>\n",
       "      <td>1519842.125</td>\n",
       "      <td>6.167424e+05</td>\n",
       "      <td>466.696869</td>\n",
       "      <td>6.167419e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.521989e+06</td>\n",
       "      <td>449.648376</td>\n",
       "      <td>1521988.875</td>\n",
       "      <td>5.793311e+05</td>\n",
       "      <td>438.313385</td>\n",
       "      <td>5.793309e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.564564e+06</td>\n",
       "      <td>433.162140</td>\n",
       "      <td>1564563.500</td>\n",
       "      <td>5.502051e+05</td>\n",
       "      <td>418.092773</td>\n",
       "      <td>5.502052e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.550821e+06</td>\n",
       "      <td>416.662537</td>\n",
       "      <td>1550820.500</td>\n",
       "      <td>5.263106e+05</td>\n",
       "      <td>400.856995</td>\n",
       "      <td>5.263106e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.571377e+06</td>\n",
       "      <td>406.756470</td>\n",
       "      <td>1571376.250</td>\n",
       "      <td>5.063670e+05</td>\n",
       "      <td>388.288727</td>\n",
       "      <td>5.063671e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.628801e+06</td>\n",
       "      <td>399.145172</td>\n",
       "      <td>1628800.375</td>\n",
       "      <td>4.919880e+05</td>\n",
       "      <td>379.977386</td>\n",
       "      <td>4.919880e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.623107e+06</td>\n",
       "      <td>391.227875</td>\n",
       "      <td>1623106.125</td>\n",
       "      <td>4.802501e+05</td>\n",
       "      <td>373.739136</td>\n",
       "      <td>4.802500e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.630332e+06</td>\n",
       "      <td>386.713501</td>\n",
       "      <td>1630331.500</td>\n",
       "      <td>4.681116e+05</td>\n",
       "      <td>367.824768</td>\n",
       "      <td>4.681119e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.662621e+06</td>\n",
       "      <td>383.530792</td>\n",
       "      <td>1662621.000</td>\n",
       "      <td>4.597879e+05</td>\n",
       "      <td>362.739166</td>\n",
       "      <td>4.597879e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.759382e+06</td>\n",
       "      <td>384.897675</td>\n",
       "      <td>1759382.375</td>\n",
       "      <td>4.507183e+05</td>\n",
       "      <td>359.003693</td>\n",
       "      <td>4.507185e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.725214e+06</td>\n",
       "      <td>375.179016</td>\n",
       "      <td>1725214.875</td>\n",
       "      <td>4.452903e+05</td>\n",
       "      <td>356.093536</td>\n",
       "      <td>4.452900e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.742416e+06</td>\n",
       "      <td>372.376801</td>\n",
       "      <td>1742416.250</td>\n",
       "      <td>4.379746e+05</td>\n",
       "      <td>353.001648</td>\n",
       "      <td>4.379743e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.755290e+06</td>\n",
       "      <td>371.004425</td>\n",
       "      <td>1755290.125</td>\n",
       "      <td>4.326714e+05</td>\n",
       "      <td>349.567230</td>\n",
       "      <td>4.326713e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.756581e+06</td>\n",
       "      <td>369.340698</td>\n",
       "      <td>1756581.000</td>\n",
       "      <td>4.285299e+05</td>\n",
       "      <td>347.832245</td>\n",
       "      <td>4.285302e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        val_loss      val_mae      val_mse          loss          mae  \\\n",
       "0   3.633166e+06  1225.382935  3633165.750  1.700276e+07  2680.731934   \n",
       "1   1.757298e+06   670.334290  1757298.000  1.556326e+06   839.739807   \n",
       "2   1.625917e+06   616.744751  1625917.000  8.579323e+05   613.882446   \n",
       "3   1.583234e+06   580.409119  1583233.375  7.657270e+05   572.648682   \n",
       "4   1.559926e+06   540.789917  1559926.500  7.087892e+05   534.118408   \n",
       "5   1.548469e+06   507.923553  1548470.125  6.590870e+05   497.728638   \n",
       "6   1.519842e+06   471.801086  1519842.125  6.167424e+05   466.696869   \n",
       "7   1.521989e+06   449.648376  1521988.875  5.793311e+05   438.313385   \n",
       "8   1.564564e+06   433.162140  1564563.500  5.502051e+05   418.092773   \n",
       "9   1.550821e+06   416.662537  1550820.500  5.263106e+05   400.856995   \n",
       "10  1.571377e+06   406.756470  1571376.250  5.063670e+05   388.288727   \n",
       "11  1.628801e+06   399.145172  1628800.375  4.919880e+05   379.977386   \n",
       "12  1.623107e+06   391.227875  1623106.125  4.802501e+05   373.739136   \n",
       "13  1.630332e+06   386.713501  1630331.500  4.681116e+05   367.824768   \n",
       "14  1.662621e+06   383.530792  1662621.000  4.597879e+05   362.739166   \n",
       "15  1.759382e+06   384.897675  1759382.375  4.507183e+05   359.003693   \n",
       "16  1.725214e+06   375.179016  1725214.875  4.452903e+05   356.093536   \n",
       "17  1.742416e+06   372.376801  1742416.250  4.379746e+05   353.001648   \n",
       "18  1.755290e+06   371.004425  1755290.125  4.326714e+05   349.567230   \n",
       "19  1.756581e+06   369.340698  1756581.000  4.285299e+05   347.832245   \n",
       "\n",
       "             mse  \n",
       "0   1.700277e+07  \n",
       "1   1.556325e+06  \n",
       "2   8.579321e+05  \n",
       "3   7.657268e+05  \n",
       "4   7.087892e+05  \n",
       "5   6.590866e+05  \n",
       "6   6.167419e+05  \n",
       "7   5.793309e+05  \n",
       "8   5.502052e+05  \n",
       "9   5.263106e+05  \n",
       "10  5.063671e+05  \n",
       "11  4.919880e+05  \n",
       "12  4.802500e+05  \n",
       "13  4.681119e+05  \n",
       "14  4.597879e+05  \n",
       "15  4.507185e+05  \n",
       "16  4.452900e+05  \n",
       "17  4.379743e+05  \n",
       "18  4.326713e+05  \n",
       "19  4.285302e+05  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>val_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4123.441766</td>\n",
       "      <td>1906.086501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1247.527455</td>\n",
       "      <td>1325.631170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>926.246255</td>\n",
       "      <td>1275.114505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>875.058177</td>\n",
       "      <td>1258.266019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>841.896186</td>\n",
       "      <td>1248.970176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>811.841502</td>\n",
       "      <td>1244.375396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>785.329191</td>\n",
       "      <td>1232.818772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>761.137923</td>\n",
       "      <td>1233.689132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>741.758215</td>\n",
       "      <td>1250.825128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>725.472648</td>\n",
       "      <td>1245.319437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>711.594776</td>\n",
       "      <td>1253.545472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>701.418563</td>\n",
       "      <td>1276.244638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>693.000699</td>\n",
       "      <td>1274.011823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>684.187018</td>\n",
       "      <td>1276.844352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>678.076646</td>\n",
       "      <td>1289.426617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>671.355741</td>\n",
       "      <td>1326.417120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>667.300555</td>\n",
       "      <td>1313.474353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>661.796277</td>\n",
       "      <td>1320.006155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>657.777532</td>\n",
       "      <td>1324.873626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>654.622172</td>\n",
       "      <td>1325.360706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse     val_rmse\n",
       "0   4123.441766  1906.086501\n",
       "1   1247.527455  1325.631170\n",
       "2    926.246255  1275.114505\n",
       "3    875.058177  1258.266019\n",
       "4    841.896186  1248.970176\n",
       "5    811.841502  1244.375396\n",
       "6    785.329191  1232.818772\n",
       "7    761.137923  1233.689132\n",
       "8    741.758215  1250.825128\n",
       "9    725.472648  1245.319437\n",
       "10   711.594776  1253.545472\n",
       "11   701.418563  1276.244638\n",
       "12   693.000699  1274.011823\n",
       "13   684.187018  1276.844352\n",
       "14   678.076646  1289.426617\n",
       "15   671.355741  1326.417120\n",
       "16   667.300555  1313.474353\n",
       "17   661.796277  1320.006155\n",
       "18   657.777532  1324.873626\n",
       "19   654.622172  1325.360706"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_metrics_df = history_df[[\"mse\", \"val_mse\"]].apply(np.sqrt)\n",
    "root_metrics_df.rename({\"mse\":\"rmse\", \"val_mse\":\"val_rmse\"}, axis=1, inplace=True)\n",
    "root_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.Figure(figsize=(14,6), dpi=100)\n",
    "\n",
    "plt.plot(root_metrics_df[\"rmse\"], label = 'Training error')\n",
    "plt.plot(root_metrics_df[\"val_rmse\"], label = 'Validation error')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "# plt.xlim([0, epochs])\n",
    "plt.xticks(range(1,20))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "X_test_scaled = st_scaler.transform(X_test)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  805.534493752449\n",
      "MAE:  351.5769102798818\n"
     ]
    }
   ],
   "source": [
    "# Report regression performance on test set\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"MAE: \", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
